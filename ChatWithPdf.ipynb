{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6vg1ANwlo_H",
        "outputId": "27e76576-3868-48c6-d906-9c8db1d0aa3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "# !pip install sentence_transformers\n",
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "import google.generativeai as palm\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "L6Sr_WvOLpqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KA0vrZMMmBn-"
      },
      "outputs": [],
      "source": [
        "\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Open a PDF file\n",
        "pdf_file_path = '/content/drive/MyDrive/Attention All You Need.pdf'\n",
        "pdf_reader = PdfReader(pdf_file_path)\n",
        "\n",
        "# Initialize an empty list to store the extracted text\n",
        "pdf_text = []\n",
        "\n",
        "# Loop through each page and extract text\n",
        "for page in pdf_reader.pages:\n",
        "    pdf_text.append(page.extract_text())\n",
        "\n",
        "# Now, you can use pdf_text as your documents\n",
        "documents = pdf_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uct_03rBllTh",
        "outputId": "9a5c93f1-aac9-4adc-d7fc-161dc7e33f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Querywhat is attention function\n",
            "Top similar sentences:\n",
            "Scaled Dot-Product Attention\n",
            " Multi-Head Attention\n",
            "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
            "attention layers running in parallel.\n",
            "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
            "query with the corresponding key.\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
            "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
            "query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\n",
            "values.\n",
            "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
            "into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\n",
            "the matrix of outputs as:\n",
            "Attention( Q, K, V ) = softmax(QKT\n",
            "√dk)V (1)\n",
            "The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\n",
            "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
            "of1√dk. Additive attention computes the compatibility function using a feed-forward network with\n",
            "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
            "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
            "matrix multiplication code.\n",
            "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\n",
            "dot product attention without scaling for larger values of dk[3]. We suspect that for large values of\n",
            "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
            "extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\n",
            "3.2.2 Multi-Head Attention\n",
            "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
            "we found it beneficial to linearly project the queries, keys and values htimes with different, learned\n",
            "linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\n",
            "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
            "4To illustrate why the dot products get large, assume that the components of qandkare independent random\n",
            "variables with mean 0and variance 1. Then their dot product, q·k=Pdk\n",
            "i=1qiki, has mean 0and variance dk.\n",
            "4: Similarity Score = 0.4916\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load a pre-trained sentence embedding model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Sample list of sentences\n",
        "sentences = documents\n",
        "\n",
        "# User input sentence\n",
        "user_input = input(\"Input Query\")\n",
        "\n",
        "# Encode sentences and user input\n",
        "sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "user_input_embedding = model.encode(user_input, convert_to_tensor=True)\n",
        "\n",
        "# Calculate cosine similarities between user input and sentences\n",
        "cosine_scores = util.pytorch_cos_sim(user_input_embedding, sentence_embeddings)\n",
        "\n",
        "# Sort sentences by similarity score\n",
        "sorted_scores, sorted_indices = cosine_scores[0].sort(descending=True)\n",
        "\n",
        "# Print top similar sentences\n",
        "print(\"Top similar sentences:\")\n",
        "for i in range(1):  # Adjust the number of top similar sentences you want to display\n",
        "    similar_sentence_index = sorted_indices[i]\n",
        "    similar_sentence = sentences[similar_sentence_index]\n",
        "    similarity_score = sorted_scores[i]\n",
        "    print(f\"{similar_sentence}: Similarity Score = {similarity_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTZjgCzXuUtY"
      },
      "outputs": [],
      "source": [
        "user_text='what is attention function'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EXImcKtxtrEI"
      },
      "outputs": [],
      "source": [
        "similar_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "LNCabty2lxh0",
        "outputId": "84f833a9-422d-42a3-fba5-11c22bd8849d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Attention is a mechanism that enables a neural network to focus on different parts of an input sequence. It does this by calculating a score for each possible alignment between the input sequence and a query, and then using those scores to weight the values of the input sequence. This allows the network to attend to the most relevant parts of the input sequence for a given task.  Scaled dot-product attention is a particular type of attention that is commonly used in natural language processing tasks. It works by calculating the dot product between a query and each of the keys in the input sequence, and then dividing each dot product by the square root of the dimension of the keys. This is done to prevent the dot products from becoming too large, which can cause the softmax function to have extremely small gradients.  Multi-head attention is a technique that is used to improve the performance of attention models. It works by performing multiple attention passes, each with a different set of keys and values. This allows the network to learn different aspects of the input sequence, which can improve its performance on a variety of tasks.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !pip install google-generativeai\n",
        "import google.generativeai as palm\n",
        "messages=['hello answer the question with the reference contextgiven by the user']\n",
        "\n",
        "palm.configure(api_key=\"AIzaSyCpzY5teMmWo8xQYQtFdTvNK72d9JgHEXs\")\n",
        "\n",
        "defaults = {\n",
        "  'model': 'models/chat-bison-001',\n",
        "  'temperature': 0.9,\n",
        "  'candidate_count': 1,\n",
        "  'top_k': 40,\n",
        "  'top_p': 0.95,\n",
        "}\n",
        "context = \"be a question answer solver assistant which answers the given questions with the given reference context only\"\n",
        "examples = [\n",
        "  [  \"hello answer the question with the reference in the given context\",\n",
        "      \"ya sure , give me the reference context\"\n",
        "  ],\n",
        "  [\n",
        "      'who is jack , reference context - jack was a fruit seller in the tokyo',\n",
        "      'jack was a fruit seller'\n",
        "  ]\n",
        "]\n",
        "def lang_model(question):\n",
        "  messages.append(question)\n",
        "  response = palm.chat(\n",
        "      **defaults,\n",
        "      context=context,\n",
        "      examples=examples,\n",
        "      messages=messages)\n",
        "  g=response.last # Response of the AI to your most recent request\n",
        "  def replace(string, replacements):\n",
        "    new_string = \"\"\n",
        "    for character in string:\n",
        "      if character in replacements:\n",
        "        new_string += replacements[character]\n",
        "      else:\n",
        "        new_string += character\n",
        "    return new_string\n",
        "  string=g\n",
        "  replacements = {\"\\n\" : \" \", \"\\r\" : \" \", \"\\s\" : \" \",\"\\t\":\" \"}\n",
        "  new_string = replace(string,replacements)\n",
        "  messages.append(new_string)\n",
        "  return new_string\n",
        "lang_model(f'answer the question in 50 words with given context ,question-{user_text},reference_context-{similar_sentence}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9rNOLSJrCam"
      },
      "outputs": [],
      "source": [
        "def ChatWithPdf(file_path,input):\n",
        "      # Open a PDF file\n",
        "      pdf_file_path = '/content/drive/MyDrive/Attention All You Need.pdf'\n",
        "      pdf_reader = PdfReader(pdf_file_path)\n",
        "\n",
        "      # Initialize an empty list to store the extracted text\n",
        "      pdf_text = []\n",
        "\n",
        "      # Loop through each page and extract text\n",
        "      for page in pdf_reader.pages:\n",
        "          pdf_text.append(page.extract_text())\n",
        "\n",
        "      # Now, you can use pdf_text as your documents\n",
        "      documents = pdf_text\n",
        "      # Load a pre-trained sentence embedding model\n",
        "      model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "      # Sample list of sentences\n",
        "      sentences = documents\n",
        "\n",
        "      #  User input sentence\n",
        "      user_input = f\"{input}\"\n",
        "\n",
        "      # Encode sentences and user input\n",
        "      sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "      user_input_embedding = model.encode(user_input, convert_to_tensor=True)\n",
        "\n",
        "      # Calculate cosine similarities between user input and sentences\n",
        "      cosine_scores = util.pytorch_cos_sim(user_input_embedding, sentence_embeddings)\n",
        "\n",
        "      # Sort sentences by similarity score\n",
        "      sorted_scores, sorted_indices = cosine_scores[0].sort(descending=True)\n",
        "      for i in range(1):  # Adjust the number of top similar sentences you want to display\n",
        "          similar_sentence_index = sorted_indices[i]\n",
        "          similar_sentence = sentences[similar_sentence_index]\n",
        "          similarity_score = sorted_scores[i]\n",
        "          # print(f\"{similar_sentence}: Similarity Score = {similarity_score:.4f}\")\n",
        "      messages=['hello answer the question with the reference contextgiven by the user']\n",
        "\n",
        "      palm.configure(api_key=\"AIzaSyCpzY5teMmWo8xQYQtFdTvNK72d9JgHEXs\")\n",
        "\n",
        "      defaults = {\n",
        "        'model': 'models/chat-bison-001',\n",
        "        'temperature': 0.9,\n",
        "        'candidate_count': 1,\n",
        "        'top_k': 40,\n",
        "        'top_p': 0.95,\n",
        "      }\n",
        "      context = \"be a question answer solver assistant which answers the given questions with the given reference context only\"\n",
        "      examples = [\n",
        "        [  \"hello answer the question with the reference in the given context\",\n",
        "            \"ya sure , give me the reference context\"\n",
        "        ],\n",
        "        [\n",
        "            'who is jack , reference context - jack was a fruit seller in the tokyo',\n",
        "            'jack was a fruit seller'\n",
        "        ]\n",
        "      ]\n",
        "      def lang_model(question):\n",
        "        messages.append(question)\n",
        "        response = palm.chat(\n",
        "            **defaults,\n",
        "            context=context,\n",
        "            examples=examples,\n",
        "            messages=messages)\n",
        "        g=response.last # Response of the AI to your most recent request\n",
        "        def replace(string, replacements):\n",
        "          new_string = \"\"\n",
        "          for character in string:\n",
        "            if character in replacements:\n",
        "              new_string += replacements[character]\n",
        "            else:\n",
        "              new_string += character\n",
        "          return new_string\n",
        "        string=g\n",
        "        replacements = {\"\\n\" : \" \", \"\\r\" : \" \", \"\\s\" : \" \",\"\\t\":\" \"}\n",
        "        new_string = replace(string,replacements)\n",
        "        messages.append(new_string)\n",
        "        return new_string\n",
        "      return lang_model(f'answer the question in 50 words with given context ,question-{user_input},reference_context-{similar_sentence}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Attention All You Need.pdf\"\n",
        "question = \"What is Recurrent Neural Network\"\n",
        "chatwithpdf = ChatWithPdf(path, question)"
      ],
      "metadata": {
        "id": "k8j_0yqXJuVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatwithpdf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8N_QcYmvKK-k",
        "outputId": "8f650f8e-725e-47eb-c291-85e58b915193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A recurrent neural network (RNN) is a type of artificial neural network that is commonly used for sequence modeling. RNNs are made up of a chain of repeating units, called neurons, that are connected to each other. Each neuron takes as input the output of the previous neuron in the chain, as well as the current input. This allows RNNs to learn long-term dependencies between inputs, which is important for many tasks such as natural language processing and machine translation.  The reference context talks about the Transformer, which is a neural network architecture that does not use recurrence. The Transformer relies entirely on attention, which is a mechanism that allows the network to attend to different parts of the input sequence. This makes the Transformer much more parallelizable than RNNs, which can be important for long sequences.  In summary, RNNs are a type of neural network that are commonly used for sequence modeling. They are made up of a chain of repeating units, called neurons, that are connected to each other. Each neuron takes as input the output of the previous neuron in the chain, as well as the current input. This allows RNNs to learn long-term dependencies between inputs, which is important for many tasks such as natural language processing and machine translation. The Transformer is a neural network architecture that does not use recurrence. It relies entirely on attention, which is a mechanism that allows the network to attend to different parts of the input sequence. This makes the Transformer much more parallelizable than RNNs, which can be important for long sequences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98Qsl9k9KiQY",
        "outputId": "1239eff5-1cc4-4b2f-ca15-d5244e7acd8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is Function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ebfdvfSKuRw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}